
The field of deep learning has enabled various breakthrough technologies in the biotech sector \citep{alphafold, wong2024discovery, cheng2023accurate}. While these technologies constitute major advancements, they investigated inputs in the form of small sections of available data due to computational constraints. In this work we take the first steps towards processing the whole human genome at once using modern deep learning techniques, while keeping the required compute to a minimum.

A major problem when analyzing human genomes is the size of the data as well as the high amount of complexity inherent in the data and unclear interaction pathways between different genomes. 

In \citet{yelmen2021creating} they use GAN networks to generate small sections of human genomes and use a variety of metrics like linkage disequilibirum and nearest neighbour adversarial accuracy to measure the quality of their creates genomes.

We postulate that by compressing the human genome using prior biological knowledge, as well as using the latest deep learning techniques,
%consisting of unsupervised learning on large scale data, 
we can solve the common problem of sharing highly sensitive data by replacing the real data with high quality synthetic data. 