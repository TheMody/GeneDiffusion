\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{alphafold,wong2024discovery,cheng2023accurate}
\citation{yelmen2021creating}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{vaswani2017attention}
\citation{nguyen2023hyenadna}
\citation{dosovitskiy2021an}
\citation{rombach2021highresolution}
\citation{ddpm,ddim}
\citation{ddpm}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Unsupervised Learning on Genetic Data}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Working with Long Sequences}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Diffusion Models}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Training}{2}{subsubsection.2.3.1}\protected@file@percent }
\citation{ddpm}
\citation{project2018project}
\citation{auer2012imputation,dolzhenko2017detection}
\newlabel{eq:nn}{{3}{3}{Training}{equation.2.3}{}}
\newlabel{eq:denoise}{{4}{3}{Training}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Data Generation}{3}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{eq:generation}{{6}{3}{Data Generation}{equation.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}\protected@file@percent }
\newlabel{sec:methods}{{3}{3}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data}{3}{subsection.3.1}\protected@file@percent }
\citation{project2018project}
\citation{capsulenet}
\citation{Unet}
\citation{bert}
\citation{vit}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Data Pre-processing}{4}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Pre-processing pipeline\relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pca}{{1}{4}{Pre-processing pipeline\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Model Architecture}{4}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A structural overview of the architecture of the MLP diffusion model.\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:unet}{{2}{4}{A structural overview of the architecture of the MLP diffusion model.\relax }{figure.caption.3}{}}
\citation{fid}
\citation{is}
\citation{yale2019privacy}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Combining Models}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Evaluation}{5}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Nearest Neighbour adversarial accuracy}{5}{subsubsection.3.5.1}\protected@file@percent }
\citation{UMAP}
\citation{tsne}
\citation{capsulenet}
\citation{capsulenet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Nearest Neighbour test}{6}{subsubsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}UMAP}{6}{subsubsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.4}Amyotrophic Lateral Sclerosis (ALS) classification}{6}{subsubsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{6}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training Metrics}{6}{subsection.4.1}\protected@file@percent }
\citation{UMAP}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Reconstruction error vs noise curves during training for different diffusion model backbones. Note the logarithmic scaling of the y-axis. \relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig:lossvsnoise}{{4}{7}{Reconstruction error vs noise curves during training for different diffusion model backbones. Note the logarithmic scaling of the y-axis. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {train loss}}}{7}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {validation loss}}}{7}{subfigure.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {train rec error}}}{7}{subfigure.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {validation rec error}}}{7}{subfigure.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {CNN}}}{7}{subfigure.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Transformer}}}{7}{subfigure.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {MLP}}}{7}{subfigure.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Accuracy on a hold out test set after training different ALS classifier (MLP, Transformer or CNN) on different synthetically generated data types (generated by: MLP, Transformer, CNN, MLP + CNN or Baseline). The best synthetic data for each classifier type is marked in \textbf  {bold}.\relax }}{7}{table.caption.7}\protected@file@percent }
\newlabel{Fig:cls}{{1}{7}{Accuracy on a hold out test set after training different ALS classifier (MLP, Transformer or CNN) on different synthetically generated data types (generated by: MLP, Transformer, CNN, MLP + CNN or Baseline). The best synthetic data for each classifier type is marked in \textbf {bold}.\relax }{table.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Top curves are smoothed loss curves for all diffusion model architectures during training on train and validation data. Bottom curves are smoothed reconstruction error e.g. $||x_p,x||$ during training and validation using single shot denoising according to Eq \ref  {eq:denoise}.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:losscurvesall}{{3}{7}{Top curves are smoothed loss curves for all diffusion model architectures during training on train and validation data. Bottom curves are smoothed reconstruction error e.g. $||x_p,x||$ during training and validation using single shot denoising according to Eq \ref {eq:denoise}.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Evaluation}{7}{subsection.4.2}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{reference}
\bibcite{project2018project}{{1}{2018}{{pro}}{{}}}
\bibcite{auer2012imputation}{{2}{2012}{{Auer et~al.}}{{Auer, Johnsen, Johnson, Logsdon, Lange, Nalls, Zhang, Franceschini, Fox, Lange, et~al.}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Result of nearest neighbour test (k=10) for all datasets. Closer to 0.5 is better. Best performance is marked in \textbf  {bold}.\relax }}{8}{table.caption.8}\protected@file@percent }
\newlabel{Fig:nntest}{{2}{8}{Result of nearest neighbour test (k=10) for all datasets. Closer to 0.5 is better. Best performance is marked in \textbf {bold}.\relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Result of Nearest Neighbour adversarial accuracy for generated datasets. Closer to 0.5 is better. Best performance is marked in \textbf  {bold}.\relax }}{8}{table.caption.9}\protected@file@percent }
\newlabel{Fig:nnacc}{{3}{8}{Result of Nearest Neighbour adversarial accuracy for generated datasets. Closer to 0.5 is better. Best performance is marked in \textbf {bold}.\relax }{table.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces UMAP visualizations of data of different origins. Red and grey points are test and train data (real). Blue is MLP, green is Baseline. Yellow, black and bright blue are MLP + CNN, Transformer and CNN respectively.\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:umap}{{5}{8}{UMAP visualizations of data of different origins. Red and grey points are test and train data (real). Blue is MLP, green is Baseline. Yellow, black and bright blue are MLP + CNN, Transformer and CNN respectively.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}\protected@file@percent }
\bibcite{cheng2023accurate}{{3}{2023}{{Cheng et~al.}}{{Cheng, Novati, Pan, Bycroft, {\v {Z}}emgulyt{\.e}, Applebaum, Pritzel, Wong, Zielinski, Sargeant, et~al.}}}
\bibcite{bert}{{4}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dolzhenko2017detection}{{5}{2017}{{Dolzhenko et~al.}}{{Dolzhenko, Van~Vugt, Shaw, Bekritsky, Van~Blitterswijk, Narzisi, Ajay, Rajan, Lajoie, Johnson, et~al.}}}
\bibcite{vit}{{6}{2020}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.}}}
\bibcite{dosovitskiy2021an}{{7}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}}}
\bibcite{fid}{{8}{2017}{{Heusel et~al.}}{{Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter}}}
\bibcite{ddpm}{{9}{2020}{{Ho et~al.}}{{Ho, Jain, and Abbeel}}}
\bibcite{alphafold}{{10}{2021}{{Jumper et~al.}}{{Jumper, Evans, Pritzel, Green, Figurnov, Ronneberger, Tunyasuvunakool, Bates, {\v {Z}}{\'\i }dek, Potapenko, et~al.}}}
\bibcite{capsulenet}{{11}{2023}{{Luo et~al.}}{{Luo, Kang, and Schönhuth}}}
\bibcite{UMAP}{{12}{2018}{{McInnes et~al.}}{{McInnes, Healy, Saul, and Großberger}}}
\bibcite{nguyen2023hyenadna}{{13}{2023}{{Nguyen et~al.}}{{Nguyen, Poli, Faizi, Thomas, Birch-Sykes, Wornow, Patel, Rabideau, Massaroli, Bengio, Ermon, Baccus, and Ré}}}
\bibcite{rombach2021highresolution}{{14}{2021}{{Rombach et~al.}}{{Rombach, Blattmann, Lorenz, Esser, and Ommer}}}
\bibcite{Unet}{{15}{2015}{{Ronneberger et~al.}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{is}{{16}{2016}{{Salimans et~al.}}{{Salimans, Goodfellow, Zaremba, Cheung, Radford, and Chen}}}
\bibcite{ddim}{{17}{2022}{{Song et~al.}}{{Song, Meng, and Ermon}}}
\bibcite{tsne}{{18}{2008}{{van~der Maaten and Hinton}}{{}}}
\bibcite{vaswani2017attention}{{19}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wong2024discovery}{{20}{2024}{{Wong et~al.}}{{Wong, Zheng, Valeri, Donghia, Anahtar, Omori, Li, Cubillos-Ruiz, Krishnan, Jin, et~al.}}}
\bibcite{yale2019privacy}{{21}{2019}{{Yale et~al.}}{{Yale, Dash, Dutta, Guyon, Pavao, and Bennett}}}
\bibcite{yelmen2021creating}{{22}{2021}{{Yelmen et~al.}}{{Yelmen, Decelle, Ongaro, Marnetto, Tallec, Montinaro, Furtlehner, Pagani, and Jay}}}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{10}
